<!DOCTYPE HTML>

<html>
	<head>
		<title>Javier Gañán</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>

	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="content">
							<div class="inner">
								<h1>Javier Gañán</h1>
								<p>Computer Vision - Machine Learning - Artificial Intelligence</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#about">About Me</a></li>
								<li><a href="#cv">CV</a></li>
								<li><a href="#publications">Publications</a></li>
								<!-- <li><a href="#youtube">YouTube</a></li> -->
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- About -->
							<article id="about">
								<h2 class="major">ABOUT ME</h2>
								<img src="images/about.jpg" alt="" />
								<p>As a passionate Computer Vision Engineer with a foundation in robotics and artificial intelligence, particularly in deep learning, my educational background includes a B.Sc. degree in electronic, robotics, and mechatronics engineering, as well as an M.Sc. degree in logic, computing, and artificial intelligence, both awarded by the University of Seville, Spain. <br><br>
									My passion lies in the development of computer vision software solutions, and I am committed to following the MLOps methodology to achieve excellence. Through the integration of my expertise in machine learning and software engineering, I streamline and optimize the deployment, monitoring, and management of machine learning models in real-world applications. I find great enthusiasm in the application of cutting-edge deep learning models to address real-world challenges, thereby harnessing the full potential of AI to create meaningful and impactful solutions.</p>
							</article>

						<!-- CV -->
							<article id="cv">
								<h2 class="major">CURRICULUM VITAE</h2>
								<h3 class="major">EXPERIENCE</h3>
								<p>2023-pres. | <b>Computer Vision Engineer</b></p>
								<p class="comment">Eviden</p>
								<p>2021-2023. | <b>Research Assistant</b></p>
								<p class="comment">GRVC Robotics Lab, Universidad de Sevilla, Spain</p>
								<p>2020-2021. | <b>Software Engineer</b></p>
								<p class="comment">Babel</p>

								<hr>

								<h3 class="major">EDUCATION</h3>
								<p>2021-2022 | <b>Master in Logic, Computing, and Artificial Intelligence</b></p>
								<p class="comment">Universidad de Sevilla, Spain</p>
								<p>2019-2020 | <b>Automation Engineering - Laurea Magistrale</b></p>
								<p class="comment">Università di Bologna, Italy</p>
								<p>2017-2021 | <b>Degree in Electronic, Robotics, and Mechatronics Engineering</b></p>
								<p class="comment">Universidad de Sevilla, Spain</p>

								<hr>

								<h3 class="major">OTHERS</h3>
								<p>2023 | <b>euROBIN Week 2023 Organizing Committee</b></p>
								<p class="comment2">euROBIN. The European Excellence Network on AI-Powered Robotics</p>
								<p>2022 | <b>SSRR 2022 Local Arrangement Committee</b></p>
								<p class="comment2">IEEE International Symposium on Safety, Security, and Rescue Robotics 2022</p>

							</article>

						<!-- Publications -->
							<article id="publications">
								<span class="close">&times;</span>
								<h2 class="major">PUBLICATIONS</h2>
								<!-- ================================================== -->
								<p><b>2024 &nbsp;|&nbsp; Modelling and Identification Methods for Simulation of Cable-Suspended Dual-Arm Robotic Systems</b></p>
								<p class="subtitle">Robotics and Autonomous Systems</p>
								<div class="button pdf" onclick="window.open('https://pdf.sciencedirectassets.com/271599/AIP/1-s2.0-S0921889024000265/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCXVzLWVhc3QtMSJGMEQCIBQWOYVAS4EX47t9uCrP0w7etvZyYrdpuOK9jqraiVHsAiAKbvGUDpy4gfeSa1%2F0iJCHzfdzzvU5pp53phQZAZnw0iqyBQhNEAUaDDA1OTAwMzU0Njg2NSIM5q7eV%2B8kYfBRQW%2B4Ko8FdjveHZZz8v0lCIi0y7maWfC5w3lVazGdcjMXHos4I0DsM%2FfDreIJxFXQWPfqR1m8HnauQ%2BTuYdWqccQN%2B7BCWWSTCaqldpewjH58PjODYgDjAl4YW4MbhcAPWrSNAeCoYAek7Trxf3BduKbBGr2Q90LozfeN%2F71VNVPge%2FOoOvvwP1pU0gH4KXRvBb9K4foq6hTwts8o5em88f4aOmgKzNls8PiJEv42CH6D9k0Js0eeLlPmHSUrPcg7ptbwLXhHCo38xemle1LVaa4ymT8Hfh35G7lIA8OzEGOufLdv3%2FhgRsHTn9uhxmICwaFWmP%2F4Q0uA96YmG%2BNONLWF5f0EIu9F30B1%2B9w0KWblo0dvUI2H4Wg4lUnnentBxfeCZe5IiTWtOVXKkczIZ3uOFgcljdrphdQUq7i%2FzttWEoT2w1DqLg%2BU3T%2B56LgbOCr%2FypxBLPccFdKRV9jJpFRWRQXBwEeBfL9oItVMpDMMndYmeKWbIlsSMHlaYNsy4tLgf6Dyvu5Xhojzcaw6Q0ac69URc3s4qLLZNava%2FOJt1pSEenPlQa5IN7NamCjGn2bzkZ70teDt36TdtE0MIkMirm%2Bclq1yTESYstrMVSMughOEAV6T8J02x1koMdd20tC6fEWU%2BX3%2FjgA%2FBm4gWtDSZf1ZU44w1Cs2FMKAkb%2B7BzkFK91JHpaDidrvDIB0J4FlyS%2BohoL33J6%2FwGmQy1xa6NmvUBC2voJQABCq6vQXcovJoOYhFXCi0fiYBtxaK99IA%2Bb2H1hN4GAJDjEWM6R8hLy1U9t%2FdQbrO3rmgzLW3llrJwe8Vs7lWUn2vMvwqrdsjfGfjxIBnD%2FiA0nOV0zZ7sbekmGp2knx4WojANGyxw3%2BIjC5z6SuBjqyAeWeZ6mgB%2FRbnUCioKIR3OiMHM%2FW88H2z0ZL4mSmIxFwdEDvlpgpJwbGfEFIyKufWZiqDcdmFAkPdejgmExRvLlrpjspshBSPdhbXawIhHwsnSihLDTObRxhByD367zm0K%2FI3gqGGUmq91db4pKkDb0UAvJBHExFeqYu26N9aNkYaoBGXcrnyWGt9Zyw2ASAYA0gubJh6hSHIde5z4YByQ3MoRZO4oRWkcgLIK%2Brhx5YUIk%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240211T202057Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYU267NKZ2%2F20240211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=1288ffbc40865950f761e9d81558c9c1914fbe47520ee9df856ca9c5110d7801&hash=a5a43c4bb2fccda629a9cb31fed78ac9a63a41df1ccf516f97bc59a9157ddbf4&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0921889024000265&tid=spdf-72999e59-8a94-443a-b544-b537f095d0e4&sid=23eeea737a02604f5078c191c6a76b053726gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1f165750510957070554&rr=853f4ba18a261bc5&cc=es', '_blank');">PDF</div>
								<div class="button cite">CITE</div>
								<div>
									<img id="image-modelling" src="images/ras.png">
									<p class="abstract">This paper proposes rigid-body modelling and identification procedures for long-reach dual-arm manipulators in a cable-suspended pendulum configuration. The proposed model relies on a virtually constrained open kinematic chain and lends itself to be simulated through the most commonly used robotic simulators without explicitly account for the cables constraints and flexibility. Moreover, a dynamic parameters identification procedure is devised to improve the simulation model fidelity and reduce the sim-to-real gap for controllers deployment. We show the capability of our model to handle different cable configurations and suspension mechanisms by customising it for two representative cable-suspended dual-arm manipulation systems: the LiCAS arms suspeded by a drone and the CRANEbot system, featuring two Pilz arms suspended by a crane. The identified dynamic models are validated by comparing their evolution with data acquired from the real systems showing a high (between 91.3% to 99.4%) correlation of the response signals. In a comparison performed with baseline pendulum models, our model increases the simulation accuracy from 64.4% to 85.9%. The simulation environment and the related controllers are released as open-source code.</p>
									<p class="bibtex">
										@article{d2024modelling,<br>
										&nbsp;&nbsp;title={Modelling and identification methods for simulation of cable-suspended dual-arm robotic systems},<br>
										&nbsp;&nbsp;author={D’Ago, Giancarlo and Selvaggio, Mario and Suarez, Alejandro and Gañán, Francisco Javier and Buonocore, Luca Rosario and Di Castro, Mario and Lippiello, Vincenzo and Ollero, Anibal and Ruggiero, Fabio},<br>
										&nbsp;&nbsp;journal={Robotics and Autonomous Systems},<br>
										&nbsp;&nbsp;pages={104643},<br>
										&nbsp;&nbsp;year={2024},<br>
										&nbsp;&nbsp;publisher={Elsevier}<br>
										}
									</p>
								</div>
								<!-- ================================================== -->
								<p><b>2023 &nbsp;|&nbsp; A Comparison between Frame-based and Event-based Cameras for Flapping-Wing Robot Perception</b></p>
								<p class="subtitle">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)</p>
								<div class="button pdf" onclick="window.open('https://doi.org/10.48550/arXiv.2309.05450', '_blank');">PDF</div>
								<div class="button cite">CITE</div>
								<div>
									<img id="sample-image" src="images/iros.png">
									<p class="abstract">Perception systems for ornithopters face severe challenges. The harsh vibrations and abrupt movements caused during flapping are prone to produce motion blur and strong lighting condition changes. Their strict restrictions in weight, size, and energy consumption also limit the type and number of sensors to mount onboard. Lightweight traditional cameras have become a standard off-the-shelf solution in many flapping-wing designs. However, bioinspired event cameras are a promising solution for ornithopter perception due to their microsecond temporal resolution, high dynamic range, and low power consumption. This paper presents an experimental comparison between frame-based and an event-based camera. Both technologies are analyzed considering the particular flapping-wing robot specifications and also experimentally analyzing the performance of well-known vision algorithms with data recorded onboard a flapping-wing robot. Our results suggest event cameras as the most suitable sensors for ornithopters. Nevertheless, they also evidence the open challenges for event-based vision on board flapping-wing robots.</p>
									<p class="bibtex">
										@inproceedings{tapia2023comparison,<br>
										&nbsp;&nbsp;author={Tapia, R. and Rodríguez-Gómez, J.P and Sanchez-Diaz, J.A. and Gañán, F.J., Rodríguez, I.G. and Luna-Santamaria, J. and Martínez-de Dios, J.R. and Ollero, A.},<br>
										&nbsp;&nbsp;booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems},<br>
										&nbsp;&nbsp;title={Experimental Energy Consumption Analysis of a Flapping-Wing Robot}, <br>
										&nbsp;&nbsp;year={2023},<br>
										&nbsp;&nbsp;doi={10.48550/arXiv.2309.05450}<br>
										}
									</p>
								</div>
								<!-- ================================================== -->
								<p><b>2022 &nbsp;|&nbsp; Efficient Event-based Intrusion Monitoring using Probabilistic Distributions</b></p>
								<p class="subtitle">IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2022)</p>
								<div class="button pdf" onclick="window.open('https://doi.org/10.1109/SSRR56537.2022.10018655', '_blank');">PDF</div>
								<div class="button cite">CITE</div>
								<div>
									<img src="images/ssrr.png">
									<p class="abstract">Autonomous intrusion monitoring in unstructured complex scenarios using aerial robots requires perception systems capable to deal with problems such as motion blur or changing lighting conditions, among others. Event cameras are neuromorphic sensors that capture per-pixel illumination changes, providing low latency and high dynamic range. This paper presents an efficient event-based processing scheme for intrusion detection and tracking onboard strict resourceconstrained robots. The method tracks moving objects using a probabilistic distribution that is updated event by event, but the processing of each event involves few low-cost operations, enabling online execution on resource-constrained onboard computers. The method has been experimentally validated in several real scenarios under different lighting conditions, evidencing its accurate performance.</p>
									<p class="bibtex">
										@inproceedings{ganan2022autonomous,<br>
										&nbsp;&nbsp;author={Gañán, F.J. and Sanchaz-Diaz, J.A. and Tapia, R. and Martínez-de Dios, J.R. and Ollero, A.},<br>
										&nbsp;&nbsp;booktitle={2022 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)},<br>
										&nbsp;&nbsp;title={Efficient Event-based Intrusion Monitoring using Probabilistic Distributions}, <br>
										&nbsp;&nbsp;year={2022},<br>
										&nbsp;&nbsp;pages={211-216},<br>
										&nbsp;&nbsp;doi={10.1109/SSRR56537.2022.10018655}<br>
										}
									</p>
								</div>
								<!-- ================================================== -->
								<p><b>2022 &nbsp;|&nbsp; Scene Recognition for Urban Search and Rescue using Global Description and Semi-Supervised Labelling</b></p>
								<p class="subtitle">IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2022)</p>
								<div class="button pdf" onclick="window.open('https://doi.org/10.1109/SSRR56537.2022.10018660', '_blank');">PDF</div>
								<div class="button cite">CITE</div>
								<div>
									<img src="images/ssrrja.png">
									<p class="abstract">Autonomous aerial robots for urban search and rescue (USAR) operations require robust perception systems for localization and mapping. Although local feature description is widely used for geometric map construction, global image descriptors leverage scene information to perform semantic localization, allowing topological maps to consider relations between places and elements in the scenario. This paper proposes a scene recognition method for USAR operations using a collaborative human-robot approach. The proposed method uses global image description to train an SVM-based classification model with semi-supervised labeled data. It has been experimentally validated in several indoor scenarios on board a multirotor robot.</p>
									<p class="bibtex">
										@inproceedings{sanchez2022scene,<br>
										&nbsp;&nbsp;author={Sanchaz-Diaz, J.A. and Gañán, F.J. and Tapia, R. and Martínez-de Dios, J.R. and Ollero, A.},<br>
										&nbsp;&nbsp;booktitle={2022 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)},<br>
										&nbsp;&nbsp;title={Scene Recognition for Urban Search and Rescue using Global Description and Semi-Supervised Labelling}, <br>
										&nbsp;&nbsp;year={2022},<br>
										&nbsp;&nbsp;pages={238-243},<br>
										&nbsp;&nbsp;doi={10.1109/SSRR56537.2022.10018660}<br>
										}
									</p>
								</div>
								<!-- ================================================== -->
								<p><b>2022 &nbsp;|&nbsp; Aerial Manipulation System for Safe Human-Robot Handover in Power Line Maintenance</b></p>
								<p class="subtitle">Robotics: Science and Systems (RSS 2022) - Workshop in Close Proximity Human-Robot Collaboration</p>
								<div class="button pdf" onclick="window.open('https://doi.org/10.5281/zenodo.7153329', '_blank');">PDF</div>
								<div class="button cite">CITE</div>
								<div>
									<img src="images/first.png">
									<p class="abstract">Human workers conducting inspection and maintenance (I&M) operations on high altitude infrastructures like power lines or industrial facilities face significant difficulties getting tools or devices once they are deployed on this kind of workspaces. In this sense, aerial manipulation robots can be employed to deliver quickly objects to the operator, considering long reach configurations to improve safety and the feeling of comfort for the operator during the handover. This paper presents a dual arm aerial manipulation robot in cable suspended configuration intended to conduct fast and safe aerial delivery, considering a human-centered approach relying on an on-board perception system in which the aerial robot accommodates its pose to the worker. Preliminary experimental results in an indoor testbed validate the proposed system design.</p>
									<p class="bibtex">
										@article{ganan2022aerial,<br>
										&nbsp;&nbsp;author={Gañán, F.J. and Suarez, A. and Tapia, R. and Martínez-de Dios, J.R. and Ollero, A.},<br>
										&nbsp;&nbsp;journal={Robotics: Science and Systems (RSS 2022) - Workshop in Close Proximity Human-Robot Collaboration},<br>
										&nbsp;&nbsp;title={Aerial Manipulation System for Safe Human-Robot Handover in Power Line Maintenance},<br>
										&nbsp;&nbsp;year={2022},<br>
										&nbsp;&nbsp;doi={10.5281/zenodo.7153329}<br>
										}
									</p>
								</div>
							</article>

						<!-- Youtube -->
							<article id="youtube">
								<h2 class="major">COMING SOON! :)</h2>
							</article>
					</div>
					<!-- Footer -->
					<footer id="footer">
						<div>
							<div><a href="mailto:fjganan14@gmail.com" target="_blank"><img src="icons/mail.png"></a></div>
							<div><a href="https://www.linkedin.com/in/francisco-javier-ganan-onieva" target="_blank"><img src="icons/linkedin.png"></a></div>
							<div><a href="https://scholar.google.com/citations?user=wAtZdOwAAAAJ&hl=en" target="_blank"><img src="icons/scholar.png"></a></div>
							<div><a href="https://github.com/javierganan99" target="_blank"><img src="icons/github.png"></a></div>
						</div>
						<p class="copyright">&copy; Javier Gañán. BASED ON A DESIGN FROM <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>
			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/cite_abstract.js"></script>


	</body>
</html>
